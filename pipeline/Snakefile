configfile: 'parameters.yaml'

from datetime import datetime
import urllib
print("Preprocessing config", flush = True)

DB_FILE = config["db_file"]
CLUSTER = ""
DIAMOND_PATH = config["diamond"]
RAXML_PATH = config["raxml"]
MUSCLE_PATH = config["muscle"]
HMMER_PATH = config["hmmer"]
MIN_IDENTITY = config["min_identity"]
CLUSTER_ALGO = config["clustering_algorithm"]
CLUSTER_SENSITIVITY = config["sensitivity"]
FAMILY_DB = config["family_db"]
FAMILY_PROFILE = FAMILY_DB.replace("seed", "hmm")
SAFETY_PATH = config["safety"]
MMSEQS_PATH = config["mmseqs"]
REF_CRITERION = config["ref_criterion"]
WORK_DIR = config["wrkdir"]
DATA_DIR = config["datadir"]
TEMP_DIR = config["tempdir"]
UNIFORM = (f"--uniform --bin_size {config['bin_size']} --bin_width {config['bin_width']}") if config["uniform"] else ""
ALPHA_SAFE = config["alpha_safe"]
BENCHMARK = config["benchmark"]

print(UNIFORM)

if CLUSTER_SENSITIVITY == "auto":
    min_id = int(MIN_IDENTITY)
    if min_id <= 25:
        CLUSTER_SENSITIVITY = "--ultra-sensitive"
    elif min_id <= 40:
        CLUSTER_SENSITIVITY = "--very-sensitive"
    elif min_id <= 90:
        CLUSTER_SENSITIVITY = "--sensitive"
    elif 90 < min_id:
        CLUSTER_SENSITIVITY = "--fast"
    else:
        assert False, "Error choosing the sensitivity"


# removes path and file extension from filename
def clean_filename(filename):
    return filename.split("/")[-1].split(".")[0]

DB_FILENAME = clean_filename(DB_FILE)
WORK_DIR = os.path.join(WORK_DIR, f"{DB_FILENAME}.{MIN_IDENTITY}.{CLUSTER_ALGO}")
CLUSTER_IDS, = glob_wildcards(os.path.join(WORK_DIR, "fasta", "{id}.fasta"))

# def agg_input(wildcards):
#     checkpoint_output = checkpoints.separate_clusters.get(**wildcards).output[1]
#     CLUSTER_IDS, = glob_wildcards(os.path.join(checkpoint_output, "{id}.clean.fasta"))
#     a = expand(os.path.join(WORK_DIR, "fasta", "{i}.fasta"), i=CLUSTER_IDS)
#     return a

def get_timestamp():
    return datetime.today().strftime('%Y-%m-%dT%H:%M')

print(f"WORK DIR: {WORK_DIR}")
print(f"DATA DIR: {DATA_DIR}")
print(f"TEMP DIR: {TEMP_DIR}")
LOG_DIR = os.path.join("logs", get_timestamp(), "%j.out")
print(f"LOG DIR: {LOG_DIR}")
print("Finished config preprocessing", flush = True)



rule all:
    input:
        expand(os.path.join(WORK_DIR, "safety", "{id}.out"), id=CLUSTER_IDS),
        expand(os.path.join(WORK_DIR, "phmmer", "{id}.out"), id=CLUSTER_IDS),
        expand(os.path.join(WORK_DIR, "hmmsearch", "{id}.out"), id=CLUSTER_IDS),
        expand(os.path.join(WORK_DIR, "hmmscan", "{id}.out"), id=CLUSTER_IDS),
        expand(os.path.join(WORK_DIR, "id", "{id}.out"), id=CLUSTER_IDS),


# def get_ref_reqs(a):
#     if REF_CRITERION == "--identity" or REF_CRITERION == "--highlow":
#         return expand(os.path.join(WORK_DIR, "id", "{id}.out"), id=CLUSTER_IDS),
#     elif REF_CRITERION == "--similarity":
#         return expand(os.path.join(WORK_DIR, "hmmsearch", "{id}.out"), id=CLUSTER_IDS),
#     return ""

def get_ref_reqs():
    if REF_CRITERION == "--identity" or REF_CRITERION == "--highlow":
        return "id"
    elif REF_CRITERION == "--similarity":
        return "hmmsearch"
    return ""

rule change_ref:
    input:
        db = DB_FILE,
        path = WORK_DIR,
        req = WORK_DIR + "/" + get_ref_reqs()
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 2048,
        time = "02:00:00",
        queue = "short"
    output:
        temp(os.path.join(WORK_DIR, "validate_ref.txt"))
    shell:
        "python3 scripts/change_ref.py {input.db} {input.path} {REF_CRITERION} | tee {output}"



def estimate_mem(filename,  wildcards, attempt):
    with open(filename, "r") as f:
        f.readline()
        t = len(f.readline())
        return int(min(t * t * 0.0015 + 128, 14000)) * attempt

def estimate_cpu_time(filename, wildcards, attempt):
    with open(filename, "r") as f:
        f.readline()
        k = len(f.readline())
        t = min(k * k / 1000 * 0.3 + 5, 500) * attempt
        return f"{int(t/60):02}:{int(t%60):02}:00"

def get_ref(wildcards, attempt):
    path = os.path.join(WORK_DIR, "refs", wildcards.id + ".ref.fasta")
    with open(path, "r") as f:
        return f.readline().split(" ")[0][1:].split("|")[1]
         
# turns 0.75f into ".a75"
ALPHA_SAFE_S = "a" + f"{ALPHA_SAFE:.2f}".split(".")[-1]

rule safe:
    input:
        expand(os.path.join(WORK_DIR, f"safety.{ALPHA_SAFE_S}", "{id}.out"), id=CLUSTER_IDS)

rule _safe:
    input:
        fasta = ancient(os.path.join(WORK_DIR, "fasta", "{id}.fasta"))
    output:
        out = os.path.join(WORK_DIR, f"safety.{ALPHA_SAFE_S}", "{id}.out"),
        benchmark = os.path.join(WORK_DIR, f"safety.{ALPHA_SAFE_S}", f"benchmark", "{id}.out") if BENCHMARK else []
    threads: 1
    resources:
        mem_mb = lambda wildcards, attempt, input: estimate_mem(str(input), wildcards, attempt),
        time = lambda wildcards, attempt, input: estimate_cpu_time(str(input), wildcards, attempt),
        ref = lambda wildcards, attempt, input: get_ref(wildcards, attempt),
        logdir = LOG_DIR,
        queue = "short",
    shell:
        "/usr/bin/time -v {SAFETY_PATH} -f {input.fasta} --threads {threads} --alpha {ALPHA_SAFE} -r \"{resources.ref}\" > {output.out} 2> {output.benchmark}" if BENCHMARK else "{SAFETY_PATH} -f {input.fasta} --threads {threads} --alpha {ALPHA_SAFE} -r \"{resources.ref}\" > {output.out}"

        # '''
        # if [ {BENCHMARK} == True ]
        # then
        #     /usr/bin/time -v {SAFETY_PATH} -f {input.fasta} --threads {threads} --alpha {ALPHA_SAFE} -r \"{resources.ref}\" > {output.out}
        # else
        #     {SAFETY_PATH} -f {input.fasta} --threads {threads} --alpha {ALPHA_SAFE} -r \"{resources.ref}\" > {output.out}
        # fi
        # '''

rule identity:
    input:
        ancient(expand(os.path.join(WORK_DIR, "id", "{id}.out"), id=CLUSTER_IDS))

rule _identity:
    input:
        msa = ancient(os.path.join(WORK_DIR, "msa", "{id}.fasta"))
    output:
        os.path.join(WORK_DIR, "id", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:05:00",
        queue = "short"
    shell:
        "{HMMER_PATH}/easel/miniapps/esl-alipid {input.msa} > {output}"

# search sequences agaianst a pfam domain database
rule hmmscan:
    input:
        ancient(expand(os.path.join(WORK_DIR, "hmmscan", "{id}.out"), id=CLUSTER_IDS))

rule _hmmscan:
    input:
        FAMILY_PROFILE + ".h3f",
        FAMILY_PROFILE + ".h3i",
        FAMILY_PROFILE + ".h3m",
        FAMILY_PROFILE + ".h3p",
        profile = FAMILY_PROFILE,
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta"))
    output:
        os.path.join(WORK_DIR, "hmmscan", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = lambda wildcards, attempt, input: 512 * attempt,
        time = lambda wildcards, attempt, input: f"00:{15*attempt:02}:00",
        queue = "short"
    shell:
        "{HMMER_PATH}/src/hmmscan --tblout {output} --noali {input.profile} {input.fasta} > /dev/null"

def mkdir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def parse_fasta(protein_fasta):
    id = protein_fasta.split()[0]
    sequence = ''.join(protein_fasta.split("\n")[1:])
    return (id, sequence)
    
rule pdb:
    input:
        expand(os.path.join(WORK_DIR, "pdb", "{id}"), id=CLUSTER_IDS)

rule _pdb:
    input:
        fasta = os.path.join(WORK_DIR, "fasta", "{id}.fasta")
    output:
        directory(os.path.join(WORK_DIR, "pdb", "{id}"))
    run:
        mkdir(os.path.join(WORK_DIR, "pdb", wildcards.id))
        with open(input.fasta, "r") as f:
            db_fasta = ("\n" + f.read()).split("\n>")[1:]
            for protein in db_fasta:
                accession_id = parse_fasta(protein)[0].split(":")[1]
                print(f"https://alphafold.ebi.ac.uk/files/{accession_id}-model_v1.pdb")
                urllib.request.urlretrieve(
                    f"https://alphafold.ebi.ac.uk/files/{accession_id}-model_v1.pdb",
                    os.path.join(WORK_DIR, "pdb", wildcards.id, f"{accession_id}-model_v1.pdb")
                )

# search sequences agaianst a pfam domain database
rule hmmscan_align:
    input:
        ancient(expand(os.path.join(WORK_DIR, "hmmscan_align", "{id}.out"), id=CLUSTER_IDS))

rule _hmmscan_align:
    input:
        FAMILY_PROFILE + ".h3f",
        FAMILY_PROFILE + ".h3i",
        FAMILY_PROFILE + ".h3m",
        FAMILY_PROFILE + ".h3p",
        profile = FAMILY_PROFILE,
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta"))
    output:
        os.path.join(WORK_DIR, "hmmscan_align", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = lambda wildcards, attempt, input: 512 * attempt,
        time = lambda wildcards, attempt, input: f"00:{10*attempt:02}:00",
        queue = "short"
    shell:
        "{HMMER_PATH}/src/hmmscan -o {output} {input.profile} {input.fasta}"

rule build_pfam:
    input:
        db = FAMILY_DB,
    output:
        profile = FAMILY_PROFILE
    threads: 8
    resources:
        logdir = LOG_DIR,
        cpus = 8,
        mem_mb = 1024 * 8,
        time = "01:00:00",
        queue = "short",
    shell:
        "{HMMER_PATH}/src/hmmbuild --cpu {threads} {output.profile} {input.db}"


rule profile_pfam:
    input:
        profile = FAMILY_PROFILE
    output:
        FAMILY_PROFILE + ".h3f",
        FAMILY_PROFILE + ".h3i",
        FAMILY_PROFILE + ".h3m",
        FAMILY_PROFILE + ".h3p"
    threads: workflow.cores
    resources:
        mem_mb = 1024 * 8,
        time = "01:00:00",
        queue = "short",
    shell:
        "{HMMER_PATH}/src/hmmpress {input.profile}"

rule jackhmmer:
    input:
        expand(os.path.join(WORK_DIR, "jackhmmer", "{id}.out"), id=CLUSTER_IDS)
    
rule _jackhmmer:
    input:
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta")),
        db = DB_FILE
    output:
        os.path.join(WORK_DIR, "jackhmmer", "{id}.out")
    threads: 1
    resources:
        logdir = LOG_DIR,
        mem_mb = 256,
        time = "00:15:00",
        cpus = 1,
        queue = "short"
    shell:
        "{HMMER_PATH}/src/jackhmmer --tblout {output} --noali {input.fasta} {input.db} > /dev/null"

# phmmer for all clusters
rule phmmer:
    input:
        expand(os.path.join(WORK_DIR, "phmmer", "{id}.out"), id=CLUSTER_IDS)

rule _phmmer:
    input:
        ref = ancient(os.path.join(WORK_DIR, "refs", "{id}.ref.fasta")),
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta")),
    output:
        os.path.join(WORK_DIR, "phmmer", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 256,
        time = "00:05:00",
        cpus = 1,
        queue = "short"
    shell:
        "{HMMER_PATH}/src/phmmer --tblout {output} --noali {input.ref} {input.fasta} > /dev/null"

# hmmsearch for all clusters
rule hmmsearch:
    input:
        ancient(expand(os.path.join(WORK_DIR, "hmmsearch", "{id}.out"), id=CLUSTER_IDS))

rule _hmmsearch:
    input:
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta")),
        hmm = ancient(os.path.join(WORK_DIR, "profiles", "{id}.hmm"))
    output:
        os.path.join(WORK_DIR, "hmmsearch", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:10:00",
        cpus = 1,
        queue = "short"
    shell:
        "{HMMER_PATH}/src/hmmsearch --tblout {output} --noali {input.hmm} {input.fasta} > /dev/null"

# creates needed hmm-profiles for hmmsearch
rule hmmbuild:
    input:
        expand(os.path.join(WORK_DIR, "profiles", "{id}.hmm"), id=CLUSTER_IDS)

rule:
    input:
        ancient(os.path.join(WORK_DIR, "msa", "{id}.fasta"))
    output:
        temp(os.path.join(WORK_DIR, "profiles", "{id}.hmm"))
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:10:00",
        cpus = 1,
        queue = "short"
    shell:
        "{HMMER_PATH}/src/hmmbuild {output} {input}"

# MSA for all clusters with Muscle
rule msa:
    input:
        ancient(expand(os.path.join(WORK_DIR, "msa", "{id}.fasta"), id=CLUSTER_IDS))

rule _msa:
    input:
        ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta"))
    output:
        os.path.join(WORK_DIR, "msa", "{id}.fasta")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 1024,
        time = "00:30:00",
        cpus = 1,
        queue = "short"
    shell:
        "{MUSCLE_PATH} -in {input} -out {output} -quiet"

# Separates all clusters (that fit the threshold) into separate fasta-files
checkpoint separate_clusters:
    input:
        f"{DB_FILE}",
        os.path.join(WORK_DIR, f"{DB_FILENAME}.{CLUSTER_ALGO}")
    output:
        os.path.join(WORK_DIR, "info.txt"),
        directory(os.path.join(WORK_DIR, "clean")),
        directory(os.path.join(WORK_DIR, "fasta")),
        directory(os.path.join(WORK_DIR, "refs"))
    params:
        n = config["cluster_number"],
        min_size = config["cluster_min_size"],
        max_size = config["cluster_max_size"]
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 1024,
        time = "00:30:00",
        queue = "short"
    shell:
        "python3 scripts/clusteread.py a {input} --n {params.n} --min {params.min_size} --max {params.max_size} {UNIFORM}"

CLUSTER_REQS = ""
if CLUSTER_ALGO == "mcl" or CLUSTER_ALGO == "multi-step":
    CLUSTER_REQS = os.path.join(DATA_DIR, f"{DB_FILENAME}.dmnd")
elif CLUSTER_ALGO == "mmseqs":
    CLUSTER_REQS = os.path.join(DATA_DIR, "mmseqs", f"{DB_FILENAME}")

# Generates diamond binary database for clustering
rule cluster_reqs:
    input:
        f"{DB_FILE}"
    output:
        CLUSTER_REQS
    resources:
        logdir = LOG_DIR,
        mem_mb = 1024,
        time = "00:05:00",
        cpus = 1,
        queue = "short"
    run:
        if CLUSTER_ALGO == "mcl" or CLUSTER_ALGO == "multi-step":
            shell("{DIAMOND_PATH} makedb --in {input} -d {output} --threads {resources.cpus}")
        elif CLUSTER_ALGO == "mmseqs":
            shell("{MMSEQS_PATH} createdb {input} {output}")

# Clustering
rule cluster:
    input:
        CLUSTER_REQS
    output:
        clu = os.path.join(WORK_DIR, f"{DB_FILENAME}.{CLUSTER_ALGO}"),
    params:
        identity=MIN_IDENTITY,
        algo=CLUSTER_ALGO,
        sens=CLUSTER_SENSITIVITY,
    threads: 56
    resources:
        logdir = LOG_DIR,
        mem_mb = 18500,
        time = "02:00:00",
        cpus = 56,
        queue = "short"
    shell:
        "{DIAMOND_PATH} cluster --cluster-algo {params.algo} --id {params.identity} {params.sens} -d {input} -o {output} --threads {resources.cpus} --tmpdir {TEMP_DIR} -v"
    # run:
    #     if params.algo == "mcl" or params.algo == "multi-step":
    #         shell("{DIAMOND_PATH} cluster --cluster-algo {params.algo} --id {params.identity} --cluster-steps {params.sens} -d {input} -o {output} --threads {resources.cpus} --tmpdir {TEMP_DIR} --log")
    #     elif params.algo == "mmseqs":
    #         params.identity = str(float(params.identity) / 100.0)
    #         shell("{MMSEQS_PATH} cluster {input} {output} tmp --min-seq-id {params.identity}")  
    #         shell("{MMSEQS_PATH} createtsv {input} {input} {output} {output}")      
            

# Prints some info about one cluster
rule cluster_info:
    input:
        f"{DB_FILE}",
        os.path.join(WORK_DIR, f"{DB_FILENAME}.{CLUSTER_ALGO}")
    params:
        cluster_name=CLUSTER
    shell:
        "python3 scripts/clusteread.py i {input}"

# Separates one cluster with id=CLUSTER
rule separate_cluster:
    input:
        f"{DB_FILE}",
        os.path.join(WORK_DIR, f"{DB_FILENAME}.{CLUSTER_ALGO}")
    output:
        os.path.join("out", f"{CLUSTER}.fasta"),
        os.path.join("out", f"{CLUSTER}.clean.fasta")
    params:
        cluster_name=CLUSTER
    shell:
        "python3 scripts/clusteread.py w {input} --id \"{params.cluster_name}\""

# MSA for one cluster
rule muscle:
    input:
        os.path.join("out", f"{CLUSTER}.clean.fasta")
    output:
        os.path.join("out", f"{CLUSTER}.msa.fasta")
    shell:
        "{MUSCLE_PATH} -in \"{input}\" -out \"{output}\""

# Preprocessing for ML phylogenetic tree
rule raxml_check:
    input:
        os.path.join("out", f"{CLUSTER}.msa.fasta")
    output:
        os.path.join("out", f"{CLUSTER}.raxml.rba")
    params:
        cluster_name=CLUSTER
    shell:
        "{RAXML_PATH} --parse --msa \"{input}\" --model LG+G --prefix \"./out/{params.cluster_name}\""

# Builds ML phylogenetic tree
rule raxml:
    input:
        os.path.join("out", f"{CLUSTER}.raxml.rba")
    output:
        os.path.join("out", f"{CLUSTER}.raxml.bestTree")
    params:
        cluster_name=CLUSTER
    shell:
        "{RAXML_PATH} --msa \"{input}\" LG+G --prefix \"./out/{params.cluster_name}\""    

# Prints stats of the best phylogenetic tree
rule raxml_stats:
    input:
        os.path.join("out", f"{CLUSTER}.raxml.bestTree")
    shell:
        "python3 scripts/raxml_stats.py \"{input}\""

# Build taxonomic tree using taxonomic ids
# Searches for "OX=taxonomic_id", in fasta-file
rule taxtree:
    input:
        os.path.join("out", f"{CLUSTER}.fasta")
    output:
        os.path.join("out", f"{CLUSTER}.tax.tree")
    shell:
        "python3 scripts/taxtree.py \"{input}\""