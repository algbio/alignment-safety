configfile: 'parameters.yaml'

from datetime import datetime
print("Preprocessing config", flush = True)

DB_FILE = config["db_file"]
CLUSTER = ""
DIAMOND_PATH = config["diamond"]
RAXML_PATH = config["raxml"]
MUSCLE_PATH = config["muscle"]
HMMER_PATH = config["hmmer"]
MIN_IDENTITY = config["min_identity"]
CLUSTER_ALGO = config["clustering_algorithm"]
CLUSTER_SENSITIVITY = config["sensitivity"]
FAMILY_DB = config["family_db"]
FAMILY_PROFILE = FAMILY_DB.replace("seed", "hmm")
SAFETY_PATH = config["safety"]
MMSEQS_PATH = config["mmseqs"]
REF_CRITERION = config["ref_criterion"]
WORK_DIR = config["wrkdir"]
DATA_DIR = config["datadir"]
TEMP_DIR = config["tempdir"]

if CLUSTER_SENSITIVITY == "auto":
    min_id = int(MIN_IDENTITY)
    if min_id <= 25:
        CLUSTER_SENSITIVITY = "usens"
    elif min_id <= 40:
        CLUSTER_SENSITIVITY = "vsens"
    elif min_id <= 90:
        CLUSTER_SENSITIVITY = "sens"
    elif 90 < min_id:
        CLUSTER_SENSITIVITY = "fast"
    else:
        assert False, "Error choosing the sensitivity"


# removes path and file extension from filename
def clean_filename(filename):
    return filename.split("/")[-1].split(".")[0]

DB_FILENAME = clean_filename(DB_FILE)
WORK_DIR = os.path.join(WORK_DIR, f"{DB_FILENAME}.{MIN_IDENTITY}.{CLUSTER_ALGO}")
CLUSTER_IDS, = glob_wildcards(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta"))

def agg_input(wildcards):
    checkpoint_output = checkpoints.separate_clusters.get(**wildcards).output[1]
    CLUSTER_IDS, = glob_wildcards(os.path.join(checkpoint_output, "{id}.clean.fasta"))
    a = expand(os.path.join(WORK_DIR, "clean", "{i}.clean.fasta"), i=CLUSTER_IDS)
    return a

def get_timestamp():
    return datetime.today().strftime('%Y-%m-%dT%H:%M')

LOG_DIR = os.path.join("logs", get_timestamp(), "%j.out")
print("Finished config preprocessing", flush = True)
print(f"WORK DIR: {WORK_DIR}")
print(f"DATA DIR: {DATA_DIR}")
print(f"TEMP DIR: {TEMP_DIR}")
print(f"LOG DIR: {LOG_DIR}")


rule all:
    input:
        expand(os.path.join(WORK_DIR, "safety", "{id}.out"), id=CLUSTER_IDS),
        expand(os.path.join(WORK_DIR, "phmmer", "{id}.out"), id=CLUSTER_IDS),
        expand(os.path.join(WORK_DIR, "hmmsearch", "{id}.out"), id=CLUSTER_IDS),
        expand(os.path.join(WORK_DIR, "hmmscan", "{id}.out"), id=CLUSTER_IDS),
        expand(os.path.join(WORK_DIR, "id", "{id}.out"), id=CLUSTER_IDS),


# def get_ref_reqs(a):
#     if REF_CRITERION == "--identity" or REF_CRITERION == "--highlow":
#         return expand(os.path.join(WORK_DIR, "id", "{id}.out"), id=CLUSTER_IDS),
#     elif REF_CRITERION == "--similarity":
#         return expand(os.path.join(WORK_DIR, "hmmsearch", "{id}.out"), id=CLUSTER_IDS),
#     return ""

def get_ref_reqs():
    if REF_CRITERION == "--identity" or REF_CRITERION == "--highlow":
        return "id"
    elif REF_CRITERION == "--similarity":
        return "hmmsearch"
    return ""

rule change_ref:
    input:
        db = DB_FILE,
        path = WORK_DIR,
        r = WORK_DIR + "/id",
        r2 = WORK_DIR + "/hmmsearch",
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 2048,
        time = "02:00:00",
        queue = "short"
    output:
        temp(os.path.join(WORK_DIR, "validate_ref.txt"))
    shell:
        "python3 scripts/change_ref.py {input.db} {input.path} {REF_CRITERION} > {output}"

rule safe:
    input:
        expand(os.path.join(WORK_DIR, "safety", "{id}.out"), id=CLUSTER_IDS)

def estimate_mem(filename):
    with open(filename, "r") as f:
        f.readline()
        t = len(f.readline())
        return int(min(t * t * 0.0015 + 128, 10000))

def estimate_cpu_time(filename):
    with open(filename, "r") as f:
        f.readline()
        k = len(f.readline())
        t = min(k * k / 1000 * 0.03 + 5, 500)
        return f"{int(t/60):02}:{int(t%60):02}:00"

rule _safe:
    input:
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta"))
    output:
        os.path.join(WORK_DIR, "safety", "{id}.out")
    threads: workflow.cores
    resources:
        mem_mb = lambda wildcards, input: estimate_mem(str(input)),
        time = lambda wildcards, input: estimate_cpu_time(str(input)),
        logdir = LOG_DIR,
        queue = "short"
    shell:
        "{SAFETY_PATH} --threads {threads} -f {input.fasta} > {output}"

rule identity:
    input:
        ancient(expand(os.path.join(WORK_DIR, "id", "{id}.out"), id=CLUSTER_IDS))

rule _identity:
    input:
        msa = ancient(os.path.join(WORK_DIR, "msa", "{id}.fasta"))
    output:
        os.path.join(WORK_DIR, "id", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:05:00",
        queue = "short"
    shell:
        "{HMMER_PATH}/easel/miniapps/esl-alipid {input.msa} > {output}"

# search sequences agaianst a pfam domain database
rule hmmscan:
    input:
        ancient(expand(os.path.join(WORK_DIR, "hmmscan", "{id}.out"), id=CLUSTER_IDS))

rule _hmmscan:
    input:
        FAMILY_PROFILE + ".h3f",
        FAMILY_PROFILE + ".h3i",
        FAMILY_PROFILE + ".h3m",
        FAMILY_PROFILE + ".h3p",
        profile = FAMILY_PROFILE,
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta"))
    output:
        os.path.join(WORK_DIR, "hmmscan", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:05:00",
        queue = "short"
    shell:
        "{HMMER_PATH}/src/hmmscan --tblout {output} --noali {input.profile} {input.fasta} > /dev/null"

# search sequences agaianst a pfam domain database
rule hmmscan_align:
    input:
        ancient(expand(os.path.join(WORK_DIR, "hmmscan_align", "{id}.out"), id=CLUSTER_IDS))

rule _hmmscan_align:
    input:
        FAMILY_PROFILE + ".h3f",
        FAMILY_PROFILE + ".h3i",
        FAMILY_PROFILE + ".h3m",
        FAMILY_PROFILE + ".h3p",
        profile = FAMILY_PROFILE,
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta"))
    output:
        os.path.join(WORK_DIR, "hmmscan_align", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:05:00",
        queue = "short"
    shell:
        "{HMMER_PATH}/src/hmmscan -o {output} {input.profile} {input.fasta}"

rule build_pfam:
    input:
        db = FAMILY_DB,
    output:
        profile = FAMILY_PROFILE
    threads: 8
    resources:
        logdir = LOG_DIR,
        cpus = 8,
        mem_mb = 1024 * 8,
        time = "01:00:00",
        queue = "short",
    shell:
        "{HMMER_PATH}/src/hmmbuild --cpu {threads} {output.profile} {input.db}"


rule profile_pfam:
    input:
        profile = FAMILY_PROFILE
    output:
        FAMILY_PROFILE + ".h3f",
        FAMILY_PROFILE + ".h3i",
        FAMILY_PROFILE + ".h3m",
        FAMILY_PROFILE + ".h3p"
    threads: workflow.cores
    resources:
        mem_mb = 1024 * 8,
        time = "01:00:00",
        queue = "short",
    shell:
        "{HMMER_PATH}/src/hmmpress {input.profile}"

rule jackhmmer:
    input:
        expand(os.path.join(WORK_DIR, "jackhmmer", "{id}.out"), id=CLUSTER_IDS)
    
rule _jackhmmer:
    input:
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta")),
        db = DB_FILE
    output:
        os.path.join(WORK_DIR, "jackhmmer", "{id}.out")
    threads: 1
    resources:
        logdir = LOG_DIR,
        mem_mb = 256,
        time = "00:15:00",
        cpus = 1,
        queue = "short"
    shell:
        "{HMMER_PATH}/src/jackhmmer --tblout {output} --noali {input.fasta} {input.db} > /dev/null"

# phmmer for all clusters
rule phmmer:
    input:
        expand(os.path.join(WORK_DIR, "phmmer", "{id}.out"), id=CLUSTER_IDS)

rule _phmmer:
    input:
        ref = ancient(os.path.join(WORK_DIR, "refs", "{id}.ref.fasta")),
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta")),
    output:
        os.path.join(WORK_DIR, "phmmer", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 256,
        time = "00:05:00",
        cpus = 1,
        queue = "short"
    shell:
        "{HMMER_PATH}/src/phmmer --tblout {output} --noali {input.ref} {input.fasta} > /dev/null"

# hmmsearch for all clusters
rule hmmsearch:
    input:
        ancient(expand(os.path.join(WORK_DIR, "hmmsearch", "{id}.out"), id=CLUSTER_IDS))

rule _hmmsearch:
    input:
        fasta = ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta")),
        hmm = ancient(os.path.join(WORK_DIR, "profiles", "{id}.hmm"))
    output:
        os.path.join(WORK_DIR, "hmmsearch", "{id}.out")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:10:00",
        cpus = 1,
        queue = "short"
    shell:
        "{HMMER_PATH}/src/hmmsearch --tblout {output} --noali {input.hmm} {input.fasta} > /dev/null"

# creates needed hmm-profiles for hmmsearch
rule hmmbuild:
    input:
        expand(os.path.join(WORK_DIR, "profiles", "{id}.hmm"), id=CLUSTER_IDS)

rule:
    input:
        ancient(os.path.join(WORK_DIR, "msa", "{id}.fasta"))
    output:
        temp(os.path.join(WORK_DIR, "profiles", "{id}.hmm"))
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:10:00",
        cpus = 1,
        queue = "short"
    shell:
        "{HMMER_PATH}/src/hmmbuild {output} {input}"

# MSA for all clusters with Muscle
rule msa:
    input:
        ancient(expand(os.path.join(WORK_DIR, "msa", "{id}.fasta"), id=CLUSTER_IDS))

rule _msa:
    input:
        ancient(os.path.join(WORK_DIR, "clean", "{id}.clean.fasta"))
    output:
        os.path.join(WORK_DIR, "msa", "{id}.fasta")
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 512,
        time = "00:10:00",
        cpus = 1,
        queue = "short"
    shell:
        "{MUSCLE_PATH} -in {input} -out {output}"

# Separates all clusters (that fit the threshold) into separate fasta-files
checkpoint separate_clusters:
    input:
        f"{DB_FILE}",
        ancient(os.path.join(WORK_DIR, f"{DB_FILENAME}.{CLUSTER_ALGO}"))
    output:
        os.path.join(WORK_DIR, "info.txt"),
        directory(os.path.join(WORK_DIR, "clean")),
        directory(os.path.join(WORK_DIR, "fasta")),
        directory(os.path.join(WORK_DIR, "refs"))
    params:
        n = config["cluster_number"],
        min_size = config["cluster_min_size"],
        max_size = config["cluster_max_size"]
    threads: workflow.cores
    resources:
        logdir = LOG_DIR,
        mem_mb = 1024,
        time = "00:30:00",
        queue = "short"
    shell:
        "python3 scripts/clusteread.py a {input} --n {params.n} --min {params.min_size} --max {params.max_size}"

CLUSTER_REQS = ""
if CLUSTER_ALGO == "mcl" or CLUSTER_ALGO == "multi-step":
    CLUSTER_REQS = os.path.join(DATA_DIR, f"{DB_FILENAME}.dmnd")
elif CLUSTER_ALGO == "mmseqs":
    CLUSTER_REQS = os.path.join(DATA_DIR, "mmseqs", f"{DB_FILENAME}")

# Generates diamond binary database for clustering
rule cluster_reqs:
    input:
        f"{DB_FILE}"
    output:
        CLUSTER_REQS
    resources:
        logdir = LOG_DIR,
        mem_mb = 1024,
        time = "00:5:00",
        cpus = 1,
        queue = "short"
    run:
        if CLUSTER_ALGO == "mcl" or CLUSTER_ALGO == "multi-step":
            shell("{DIAMOND_PATH} makedb --in {input} -d {output} --threads {resources.cpus}")
        elif CLUSTER_ALGO == "mmseqs":
            shell("{MMSEQS_PATH} createdb {input} {output}")

# Clustering
rule cluster:
    input:
        ancient(CLUSTER_REQS)
    output:
        clu = os.path.join(WORK_DIR, f"{DB_FILENAME}.{CLUSTER_ALGO}"),
    params:
        identity=MIN_IDENTITY,
        algo=CLUSTER_ALGO,
        sens=CLUSTER_SENSITIVITY,
    resources:
        logdir = LOG_DIR,
        mem_mb = 18500,
        time = "01:00:00",
        cpus = 56,
        queue = "short"
    shell:
        "{DIAMOND_PATH} cluster --cluster-algo {params.algo} --id {params.identity} --cluster-steps {params.sens} -d {input} -o {output} --threads {resources.cpus} --tmpdir {TEMP_DIR} -v"
    # run:
    #     if params.algo == "mcl" or params.algo == "multi-step":
    #         shell("{DIAMOND_PATH} cluster --cluster-algo {params.algo} --id {params.identity} --cluster-steps {params.sens} -d {input} -o {output} --threads {resources.cpus} --tmpdir {TEMP_DIR} --log")
    #     elif params.algo == "mmseqs":
    #         params.identity = str(float(params.identity) / 100.0)
    #         shell("{MMSEQS_PATH} cluster {input} {output} tmp --min-seq-id {params.identity}")  
    #         shell("{MMSEQS_PATH} createtsv {input} {input} {output} {output}")      
            

# Prints some info about one cluster
rule cluster_info:
    input:
        f"{DB_FILE}",
        os.path.join(WORK_DIR, f"{DB_FILENAME}.{CLUSTER_ALGO}")
    params:
        cluster_name=CLUSTER
    shell:
        "python3 scripts/clusteread.py i {input}"

# Separates one cluster with id=CLUSTER
rule separate_cluster:
    input:
        f"{DB_FILE}",
        os.path.join(WORK_DIR, f"{DB_FILENAME}.{CLUSTER_ALGO}")
    output:
        os.path.join("out", f"{CLUSTER}.fasta"),
        os.path.join("out", f"{CLUSTER}.clean.fasta")
    params:
        cluster_name=CLUSTER
    shell:
        "python3 scripts/clusteread.py w {input} --id \"{params.cluster_name}\""

# MSA for one cluster
rule muscle:
    input:
        os.path.join("out", f"{CLUSTER}.clean.fasta")
    output:
        os.path.join("out", f"{CLUSTER}.msa.fasta")
    shell:
        "{MUSCLE_PATH} -in \"{input}\" -out \"{output}\""

# Preprocessing for ML phylogenetic tree
rule raxml_check:
    input:
        os.path.join("out", f"{CLUSTER}.msa.fasta")
    output:
        os.path.join("out", f"{CLUSTER}.raxml.rba")
    params:
        cluster_name=CLUSTER
    shell:
        "{RAXML_PATH} --parse --msa \"{input}\" --model LG+G --prefix \"./out/{params.cluster_name}\""

# Builds ML phylogenetic tree
rule raxml:
    input:
        os.path.join("out", f"{CLUSTER}.raxml.rba")
    output:
        os.path.join("out", f"{CLUSTER}.raxml.bestTree")
    params:
        cluster_name=CLUSTER
    shell:
        "{RAXML_PATH} --msa \"{input}\" LG+G --prefix \"./out/{params.cluster_name}\""    

# Prints stats of the best phylogenetic tree
rule raxml_stats:
    input:
        os.path.join("out", f"{CLUSTER}.raxml.bestTree")
    shell:
        "python3 scripts/raxml_stats.py \"{input}\""

# Build taxonomic tree using taxonomic ids
# Searches for "OX=taxonomic_id", in fasta-file
rule taxtree:
    input:
        os.path.join("out", f"{CLUSTER}.fasta")
    output:
        os.path.join("out", f"{CLUSTER}.tax.tree")
    shell:
        "python3 scripts/taxtree.py \"{input}\""